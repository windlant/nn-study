"""
å®Œæ•´çš„æ¨¡å‹æµ‹è¯•å’Œè¯„ä¼°è„šæœ¬
åŒ…å«ï¼šè¯¦ç»†æŒ‡æ ‡è¯„ä¼° + æ¨ç†é€Ÿåº¦æµ‹è¯• + ä¸šåŠ¡éªŒè¯
"""
import torch
import torch.nn as nn
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from src.models.mnist import SimpleCNN
import os
import matplotlib.pyplot as plt
import numpy as np
import time
import random

# å°è¯•å¯¼å…¥ sklearnï¼ˆç”¨äºè¯¦ç»†è¯„ä¼°ï¼‰
try:
    from sklearn.metrics import classification_report, precision_recall_fscore_support, confusion_matrix
    import seaborn as sns
    SKLEARN_AVAILABLE = True
except ImportError:
    print("è­¦å‘Š: sklearn æœªå®‰è£…ï¼Œå°†ä½¿ç”¨åŸºç¡€è¯„ä¼°æ–¹æ³•")
    SKLEARN_AVAILABLE = False

def load_test_data(batch_size: int = 1000):
    """åŠ è½½æµ‹è¯•æ•°æ®é›†"""
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    
    test_dataset = datasets.MNIST(
        './data', 
        train=False, 
        download=True, 
        transform=transform
    )
    
    test_loader = DataLoader(
        test_dataset, 
        batch_size=batch_size, 
        shuffle=False
    )
    
    return test_loader, test_dataset

def basic_accuracy_evaluation(model, test_loader, device):
    """åŸºç¡€å‡†ç¡®ç‡è¯„ä¼°ï¼ˆä¸ä¾èµ– sklearnï¼‰"""
    correct = 0
    total = 0
    all_predictions = []
    all_targets = []
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            pred = output.argmax(dim=1, keepdim=True)
            
            correct += pred.eq(target.view_as(pred)).sum().item()
            total += target.size(0)
            
            all_predictions.extend(pred.cpu().numpy().flatten())
            all_targets.extend(target.cpu().numpy())
    
    accuracy = 100. * correct / total
    return accuracy, all_predictions, all_targets

def detailed_evaluate_with_sklearn(predictions, targets):
    """ä½¿ç”¨ sklearn è¿›è¡Œè¯¦ç»†è¯„ä¼°"""
    if not SKLEARN_AVAILABLE:
        return None
        
    # è®¡ç®—å„ç§æŒ‡æ ‡
    accuracy = 100. * sum(np.array(predictions) == np.array(targets)) / len(targets)
    
    precision, recall, f1, _ = precision_recall_fscore_support(
        targets, predictions, average='weighted'
    )
    
    report = classification_report(
        targets, predictions, 
        target_names=[str(i) for i in range(10)],
        digits=4
    )
    
    cm = confusion_matrix(targets, predictions)
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'classification_report': report,
        'confusion_matrix': cm
    }

def plot_confusion_matrix(cm, save_path='./models/confusion_matrix.png'):
    """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
    if not SKLEARN_AVAILABLE:
        print("è·³è¿‡æ··æ·†çŸ©é˜µç»˜åˆ¶ï¼ˆsklearn æœªå®‰è£…ï¼‰")
        return
        
    plt.figure(figsize=(12, 10))
    labels = [str(i) for i in range(10)]
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=labels, yticklabels=labels)
    plt.title('Confusion Matrix - MNIST CNN', fontsize=16, fontweight='bold')
    plt.xlabel('Predicted Label', fontsize=12)
    plt.ylabel('True Label', fontsize=12)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()

def measure_inference_speed(model, device, num_samples=1000):
    """æµ‹é‡æ¨ç†é€Ÿåº¦"""
    print(f"\n=== æ¨ç†é€Ÿåº¦æµ‹è¯• ===")
    print(f"æµ‹è¯•æ ·æœ¬æ•°é‡: {num_samples}")
    
    # åˆ›å»ºéšæœºè¾“å…¥æ•°æ®
    dummy_input = torch.randn(num_samples, 1, 28, 28).to(device)
    
    # é¢„çƒ­ GPU/CPU
    model.eval()
    with torch.no_grad():
        for _ in range(10):
            _ = model(dummy_input[:10])
    
    # åŒæ­¥ GPUï¼ˆå¦‚æœä½¿ç”¨ï¼‰
    if device.type == 'cuda':
        torch.cuda.synchronize()
    
    # æµ‹é‡æ¨ç†æ—¶é—´
    start_time = time.perf_counter()
    with torch.no_grad():
        _ = model(dummy_input)
    if device.type == 'cuda':
        torch.cuda.synchronize()
    end_time = time.perf_counter()
    
    total_time = end_time - start_time
    avg_time_per_sample = (total_time / num_samples) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
    throughput = num_samples / total_time  # æ ·æœ¬/ç§’
    
    print(f"æ€»æ¨ç†æ—¶é—´: {total_time:.4f} ç§’")
    print(f"å¹³å‡æ¨ç†å»¶è¿Ÿ: {avg_time_per_sample:.2f} æ¯«ç§’/æ ·æœ¬")
    print(f"ååé‡: {throughput:.2f} æ ·æœ¬/ç§’")
    
    return {
        'total_time': total_time,
        'avg_latency_ms': avg_time_per_sample,
        'throughput': throughput
    }

def test_single_image(model, test_dataset, device, idx=None):
    """æµ‹è¯•å•å¼ å›¾åƒé¢„æµ‹"""
    if idx is None:
        idx = random.randint(0, len(test_dataset) - 1)
    
    image, true_label = test_dataset[idx]
    
    # é¢„æµ‹
    model.eval()
    with torch.no_grad():
        output = model(image.unsqueeze(0).to(device))
        predicted = output.argmax(dim=1).item()
        confidence = torch.softmax(output, dim=1).max().item()
    
    # æ˜¾ç¤ºç»“æœ
    plt.figure(figsize=(6, 4))
    plt.imshow(image.squeeze(), cmap='gray')
    plt.title(f'True: {true_label}, Predicted: {predicted}\nConfidence: {confidence:.2%}')
    plt.axis('off')
    plt.show()
    
    return true_label, predicted, confidence

def business_validation(predictions, targets):
    """ä¸šåŠ¡éªŒè¯åˆ†æ"""
    print(f"\n=== ä¸šåŠ¡éªŒè¯åˆ†æ ===")
    
    # é”™è¯¯åˆ†æ
    errors = [(i, pred, true) for i, (pred, true) in enumerate(zip(predictions, targets)) if pred != true]
    error_rate = len(errors) / len(targets) * 100
    
    print(f"é”™è¯¯ç‡: {error_rate:.2f}%")
    print(f"æ€»é”™è¯¯æ•°é‡: {len(errors)}")
    
    if len(errors) > 0:
        # æ‰¾å‡ºæœ€å¸¸è§çš„é”™è¯¯ç±»å‹
        error_pairs = [(true, pred) for _, pred, true in errors]
        from collections import Counter
        error_counter = Counter(error_pairs)
        most_common_errors = error_counter.most_common(5)
        
        print("\næœ€å¸¸è§çš„é”™è¯¯ç±»å‹ (çœŸå®æ ‡ç­¾ -> é¢„æµ‹æ ‡ç­¾):")
        for (true_label, pred_label), count in most_common_errors:
            print(f"  {true_label} -> {pred_label}: {count} æ¬¡")
    
    # æ€§èƒ½åŸºå‡†å¯¹æ¯”
    print(f"\n=== æ€§èƒ½åŸºå‡†å‚è€ƒ ===")
    print("MNIST æ•°æ®é›†åŸºå‡†æ€§èƒ½:")
    print("- ç®€å• CNN æ¨¡å‹: 98-99% å‡†ç¡®ç‡")
    print("- å¤æ‚æ¨¡å‹ (ResNetç­‰): 99.5%+ å‡†ç¡®ç‡")
    print("- äººç±»æ°´å¹³: ï½98% å‡†ç¡®ç‡")
    
    return {
        'error_rate': error_rate,
        'total_errors': len(errors),
        'common_errors': most_common_errors if errors else []
    }

def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    print("ğŸš€ å¼€å§‹æ¨¡å‹æµ‹è¯•å’Œè¯„ä¼°...")
    
    # è®¾å¤‡é…ç½®
    device = torch.device('cpu')  # æˆ– 'cuda' å¦‚æœ GPU å…¼å®¹
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ¨¡å‹
    model_path = './models/mnist_cnn.pth'
    if not os.path.exists(model_path):
        print(f"é”™è¯¯: æ¨¡å‹æ–‡ä»¶ {model_path} ä¸å­˜åœ¨!")
        return
    
    model = SimpleCNN()
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.to(device)
    print(f"æ¨¡å‹åŠ è½½æˆåŠŸ! å‚æ•°æ•°é‡: {model.count_parameters():,}")
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_loader, test_dataset = load_test_data()
    print(f"æµ‹è¯•æ•°æ®é›†å¤§å°: {len(test_dataset)}")
    
    # åŸºç¡€å‡†ç¡®ç‡è¯„ä¼°
    print("\n=== åŸºç¡€å‡†ç¡®ç‡è¯„ä¼° ===")
    accuracy, predictions, targets = basic_accuracy_evaluation(model, test_loader, device)
    print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.2f}%")
    
    # è¯¦ç»†è¯„ä¼°ï¼ˆå¦‚æœ sklearn å¯ç”¨ï¼‰
    detailed_metrics = None
    if SKLEARN_AVAILABLE:
        print("\n=== è¯¦ç»†æŒ‡æ ‡è¯„ä¼° ===")
        detailed_metrics = detailed_evaluate_with_sklearn(predictions, targets)
        if detailed_metrics is not None:
            print(f"å‡†ç¡®ç‡: {detailed_metrics['accuracy']:.2f}%")
            print(f"ç²¾ç¡®ç‡: {detailed_metrics['precision']:.4f}")
            print(f"å¬å›ç‡: {detailed_metrics['recall']:.4f}")
            print(f"F1 åˆ†æ•°: {detailed_metrics['f1']:.4f}")
            print(f"\nåˆ†ç±»æŠ¥å‘Š:\n{detailed_metrics['classification_report']}")
        
            # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
            plot_confusion_matrix(detailed_metrics['confusion_matrix'])
    else:
        print("\nè·³è¿‡è¯¦ç»†æŒ‡æ ‡è¯„ä¼°ï¼ˆsklearn æœªå®‰è£…ï¼‰")
    
    # æ¨ç†é€Ÿåº¦æµ‹è¯•
    speed_metrics = measure_inference_speed(model, device)
    
    # å•å¼ å›¾åƒæµ‹è¯•
    print(f"\n=== å•å¼ å›¾åƒé¢„æµ‹æµ‹è¯• ===")
    true_label, predicted, confidence = test_single_image(model, test_dataset, device)
    print(f"å•å¼ æµ‹è¯•ç»“æœ: çœŸå®={true_label}, é¢„æµ‹={predicted}, ç½®ä¿¡åº¦={confidence:.2%}")
    
    # ä¸šåŠ¡éªŒè¯
    business_metrics = business_validation(predictions, targets)
    
    # ä¿å­˜è¯„ä¼°ç»“æœ
    results = {
        'basic_accuracy': accuracy,
        'detailed_metrics': detailed_metrics,
        'speed_metrics': speed_metrics,
        'business_metrics': business_metrics
    }
    
    # æ‰“å°æ€»ç»“
    print(f"\n{'='*50}")
    print(f"ğŸ“Š è¯„ä¼°æ€»ç»“")
    print(f"{'='*50}")
    print(f"âœ… å‡†ç¡®ç‡: {accuracy:.2f}%")
    print(f"âš¡ å¹³å‡æ¨ç†å»¶è¿Ÿ: {speed_metrics['avg_latency_ms']:.2f} ms")
    print(f"ğŸ“ˆ ååé‡: {speed_metrics['throughput']:.2f} æ ·æœ¬/ç§’")
    print(f"âŒ é”™è¯¯ç‡: {business_metrics['error_rate']:.2f}%")
    print(f"{'='*50}")
    
    return results

if __name__ == "__main__":
    main()